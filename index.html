<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PAM: Recognize, Explain, Caption, and Segment Anything in Images and Videos">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PAM: Recognize, Explain, Caption, and Segment Anything in Images and Videos</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <link rel="icon" type="image/x-icon" href="static/images/logo.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    .easygreen { color: #009c4b; }
    .middleyellow { color: #f25922; }
    .hardred { color: #d8383a; }
  </style>

</head>
<body>

<!-- Paper Title and Author -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><img src="static/images/pam-logo.png" alt="Icon" style="height: 1.6em; vertical-align: middle; margin-right: 0.0em;"> <span class='tiifbench'>Perceive Anything Model</span></h1>
          <h2 class="subtitle is-3 has-text-centered">PAM: Recognize, Explain, Caption, and Segment<br>Anything in Images and Videos</h2> 
          <div class="is-size-5 publication-authors">
            <span class="author-block corresponding-author">
              <a href="https://afeng-x.github.io/">Weifeng Lin</a><sup>*1</sup>,
            </span>
            <span class="author-block corresponding-author">
              <a href="https://a113n-w3i.github.io/">Xinyu Wei</a><sup>*2</sup>,
            </span>
            <span class="author-block">
              <a href="">Ruichuan An</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Tianhe Ren</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="">Tingwei Chen</a><sup>1</sup>,
            </span>
            
            <span class="author-block">
              <a href="">Renrui Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Ziyi Guo</a><sup>1</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="">Wentao Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Lei Zhang</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="">Hongsheng Li</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors" style="margin-top: 10px;">
            <span class="author-block" style="margin-right: 15px;"><sup>1</sup>The Chinese University of Hong Kong</span>
            <span class="author-block" style="margin-left: 15px;"><sup>2</sup>Peking University</span>
          </div>
          <div class="is-size-5 publication-authors" style="margin-top: 10px;">
            <span class="author-block" style="margin-right: 15px;"><sup>3</sup>The University of Hong Kong</span>
            <span class="author-block" style="margin-left: 15px;"><sup>4</sup>Hong Kong Polytechnic University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv Link. -->
              <span class="link-block" style="margin-right: 15px;">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Gradio Link. -->
              <span class="link-block" style="margin-right: 15px;">
                  <a href=""
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <img src="static/images/demo.svg" alt="Model" style="width: 20px; height: 20px;">
                      </span>
                      <span>Demo</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block" style="margin-right: 15px;">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Data Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>PAM-Data</span>
                  </a>
              </span>
              <!-- MDVP-Bench Link. -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/datasets/Afeng-x/Draw-and-Understand/tree/main/MDVP-bench"
                   class="external-link button is-normal is-rounded is-dark">
                   <span class="icon">
                    <img src="static/images/wrench.svg" alt="Gradio Logo" style="width: 18px; height: 18px;">
                    </span>
                  <span>MVDP-Bench</span>
                  </a>
              </span> -->
              <!-- Model Link -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/Afeng-x/SPHINX-V-Model"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="static/images/model.svg" alt="Model" style="width: 22px; height: 22px;">
                  </span>
                  <span>Model</span>
                  </a>
              </span> -->
            </div>

          </div>
            
            
            

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- three img caption samples -->
<section class="section" style="min-height: 420px; padding-bottom: 10px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-12">
        <h2 class="title is-3 has-text-centered">
          Image Caption
        </h2>
        
        <!-- Â∫îÁî®Áº©ÊîæÂèòÊç¢‰ΩøÊï¥‰ΩìÂèòÂ§ß -->
        <div style="transform:scale(1.5);
                    transform-origin:top center;
                    width:100%;
                    overflow:visible;
                    margin-top:20px;
                    margin-bottom:0px;">
          <div class="columns is-centered is-vcentered">
            <!-- First GIF -->
            <div class="column is-4">
              <div class="content">
                <img src="static/videos/img1/img1_caption.gif" alt="Dog GIF" style="width: 100%; height: auto; border-radius: 8px;">
              </div>
            </div>
            
            <!-- Second GIF -->
            <div class="column is-4">
              <div class="content">
                <img src="static/videos/img2/img2_caption.gif" alt="Airplane GIF" style="width: 100%; height: auto; border-radius: 8px;">
              </div>
            </div>
            
            <!-- Third GIF -->
            <div class="column is-4">
              <div class="content">
                <img src="static/videos/img3/img3_caption.gif" alt="Turtle GIF" style="width: 100%; height: auto; border-radius: 8px;">
              </div>
            </div>
          </div>
        </div>
        
      </div>
    </div>
  </div>
</section>

<!-- four video caption samples -->
<section class="section" style="min-height: 420px; padding-bottom: 10px; margin-top: 40px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-12">
        <h2 class="title is-3 has-text-centered">
          Video Caption
        </h2>
        
        <!-- Â∫îÁî®Áº©ÊîæÂèòÊç¢‰ΩøÊï¥‰ΩìÂèòÂ§ß -->
        <div style="transform:scale(1.8);
                    transform-origin:top center;
                    width:100%;
                    overflow:visible;
                    margin-top:20px;
                    margin-bottom:0px;">
          
          <!-- ‰∏âÂàóÂ∏ÉÂ±ÄÔºöÁ´ñÂ±è-Ê®™Â±èÂ†ÜÂè†-Á´ñÂ±è -->
          <div class="columns is-centered">
            <!-- First Vertical Video -->
            <div class="column is-3">
              <div class="content">
                <div style="height: 400px; overflow: hidden;">
                  <img src="static/videos/vertical-video1/vertical-video1-merged.gif" alt="Vertical Video 1" style="width: 100%; height: 100%; border-radius: 2px; object-fit: cover; object-position: top;">
                </div>
              </div>
            </div>
            
            <!-- Middle Column: Two Horizontal Videos Stacked -->
            <div class="column is-4">
              <div class="content">
                <!-- First Horizontal Video -->
                <div style="margin-bottom: 50px;">
                  <img src="static/videos/hori-video1/hori-video1-merged.gif" alt="Horizontal Video 1" style="width: 100%; height: auto; border-radius: 2px;">
                </div>
                
                <!-- Second Horizontal Video -->
                <div>
                  <img src="static/videos/hori-video2/hori-video2-merged.gif" alt="Horizontal Video 2" style="width: 100%; height: auto; border-radius: 2px;">
                </div>
              </div>
            </div>
            
            <!-- Second Vertical Video -->
            <div class="column is-3">
              <div class="content">
                <div style="height: 400px; overflow: hidden;">
                  <img src="static/videos/vertical-video2/vertical-video2-merged.gif" alt="Vertical Video 2" style="width: 100%; height: 100%; border-radius: 2px; object-fit: cover; object-position: top;">
                </div>
              </div>
            </div>
          </div>
        </div>
    </div>
  </div>
</section>

<!-- three video caption samples -->
<section class="section" style="min-height: 420px; padding-bottom: 10px; margin-top: 280px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-12">
        <!-- Â∫îÁî®Áº©ÊîæÂèòÊç¢‰ΩøÊï¥‰ΩìÂèòÂ§ß -->
        <div style="transform:scale(1.5);
                    transform-origin:top center;
                    width:100%;
                    overflow:visible;
                    margin-top:20px;
                    margin-bottom:0px;">
          
          <!-- ‰∏âÂàóÂ∏ÉÂ±ÄÔºöÁ´ñÂ±è-Ê®™Â±èÂ†ÜÂè†-Á´ñÂ±è -->
          <div class="columns is-centered">
            <div class="column is-4">
              <div class="content">
                <img src="static/videos/sam2-demo1/sam2-demo1-merged.gif" alt="SAM2 Demo 1" style="width: 100%; height: auto; border-radius: 2px;">
              </div>
            </div>
            
            <!-- Second Horizontal Video -->
            <div class="column is-4">
              <div class="content">
                <img src="static/videos/sam2-demo2/sam2-demo2-merged.gif" alt="SAM2 Demo 2" style="width: 100%; height: auto; border-radius: 2px;">
              </div>
            </div>
            
            <!-- Third Horizontal Video -->
            <div class="column is-4">
              <div class="content">
                <img src="static/videos/sam2-demo3/sam2-demo3-merged.gif" alt="SAM2 Demo 3" style="width: 100%; height: auto; border-radius: 2px;">
              </div>
            </div>
          </div>
        </div>
    </div>
  </div>
</section>

<!-- one streaming caption sample -->
<section class="section" style="min-height: 300px; padding-bottom: 10px; margin-top: -80px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-12">
        <h2 class="title is-3 has-text-centered">
          Streaming Caption
        </h2>
        
        <!-- Â∫îÁî®Áº©ÊîæÂèòÊç¢‰ΩøÊï¥‰ΩìÂèòÂ§ß -->
        <div style="transform:scale(1.4);
                    transform-origin:top center;
                    width:100%;
                    overflow:visible;
                    margin-top:20px;
                    margin-bottom:0px;">
          
          <!-- Âçï‰∏™Ë∂ÖÂÆΩÊ®™Â±èËßÜÈ¢ë -->
          <div class="columns is-centered">
            <div class="column is-10">
              <div class="content">
                <img src="static/videos/stream-demo/stream-caption.gif" alt="Streaming Caption Demo" style="width: 100%; height: auto; border-radius: 4px;">
              </div>
            </div>
          </div>
          
        </div>
        
      </div>
    </div>
  </div>
</section>

<!-- Introduction -->
<section class="section" style="margin-top: 170px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            While powerful vision foundation models like Segment Anything Model 2 (SAM 2) excel at 
            object segmentation in images and videos, how to achieve deep semantic understanding of 
            these regions remains a critical challenge.
            To address this, we introduce <img src="static/images/pam-logo.png" alt="Icon" style="height: 1.6em; vertical-align: middle; margin-right: 0.0em;"> <b><span class='tiifbench'>Perceive Anything Model (PAM)</span></b>, 
            an end-to-end vision-language model that efficiently integrates SAM 2 and 
            Large Language Models (LLMs), 
            enabling the simultaneous segmentation of objects while generating diverse semantic outputs for each region in both images and videos. 
            Specifically, <img src="static/images/pam-logo.png" alt="Icon" style="height: 1.6em; vertical-align: middle; margin-right: 0.0em;"> <b>PAM</b> 
            introduces a <b>Semantic Perceiver</b> that acts as a crucial bridge. 
            This component efficiently utilizes rich intermediate features from the SAM 2 backbone, thereby incorporating general vision, localization, and semantic priors into the visual tokens, which are subsequently fed into LLMs for understanding.
          </p>
          <p>
            To ensure <img src="static/images/pam-logo.png" alt="Icon" style="height: 1.6em; vertical-align: middle; margin-right: 0.0em;"> <b>PAM</b>'s 
            robustness in understanding multi-dimensional semantic granularity, we develop a dedicated data augmentation and refinement pipeline, 
            which yields üî•<b>1.8M high-quality image data and 0.6M video data</b>.
          </p>
          <p>
            Experimental results demonstrate that, even with a lightweight 1.5/3B LLM as the semantic decoder, 
            <img src="static/images/pam-logo.png" alt="Icon" style="height: 1.6em; vertical-align: middle; margin-right: 0.0em;"> <b>PAM</b>
            achieves strong performance across a diverse range of tasks, including category prediction, brief and detailed regional captioning, 
            video captioning, and streaming region captioning. Furthermore, <img src="static/images/pam-logo.png" alt="Icon" style="height: 1.6em; vertical-align: middle; margin-right: 0.0em;"> <b>PAM</b>
             exhibits significant inference efficiency, running ‚ö°Ô∏è<b>1.2-2.4&times; faster</b> while consuming less GPU memory compared to prior approaches, marking a significant advancement for real-world applications.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Footnote for Introduction -->
<div class="container is-max-desktop" style="margin-top: -40px;">
  <div class="columns is-centered">
    <div class="column is-six-fifths">
      <div class="content has-text-left">
        <p style="font-size: 0.75em; color: #666; font-style: italic;">
          Perceive Anything is also an abbreviation for Pallas Athena. 
          In Homer's epics, Athena and her sacred owl are known for their piercing gaze, capable of perceiving all things in the world. 
          Similarly, our PAM segments every object in any visual medium and simultaneously outputs its semantic meaning.
        </p>
      </div>
    </div>
  </div>
</div>


<!-- PAM -->
<section class="section" style="margin-top: 40px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3"><img src="static/images/pam-logo.png" alt="Icon" style="height: 1.6em; vertical-align: middle; margin-right: 0.0em;"> <span class='tiifbench'>Perceive Anything Model</span></h2>
        <div class="content has-text-justified">
          <p>
            Given visual prompts such as points, boxes, or masks to specify a region of interest, <b>Perceive Anything Model (PAM)</b> can simultaneously:
          </p>
          <p>
            <i>(1) Segment:</i> Generate precise segmentation masks for the indicated region within an image or throughout a video.
          </p>
          <p>
            <i>(2) Recognize:</i> Identify the category of the designated region or object.
          </p>
          <p>
            <i>(3) Explain:</i> Provide clear explanations of the region's or object's definition, attributes, and functionality within its given context.
          </p>
          <p>
            <i>(4) Caption:</i> Generate concise or detailed captions for the region within images, videos, and video streams.
          </p>

          <h4 class="title is-4" style="margin-top: 20px;">Overall Architecture</h4>
          <p>
            <img src="static/images/pam-logo.png" alt="Icon" style="height: 1.6em; vertical-align: middle; margin-right: 0.0em;"> <b>PAM</b>'s architecture can be divided into two parts. 
            The first part is the SAM 2 framework, which comprises an image encoder, a prompt encoder, memory modules, and a mask decoder. This framework provides robust spatio-temporal visual feature extraction and segmentation capabilities. 
            The second part is a semantic decoder, which is based on a large language model (LLM).
            Crucially, our proposed Semantic Perceiver acts as a bridge, effectively leverages intermediate visual features from the SAM 2 backbone and results in visual tokens. These tokens are subsequently processed by the LLM to generate diverse semantic outputs.
            For decoding, PAM features a parallel design for its mask and semantic decoders, enabling the simultaneous segmentation of objects while generating diverse semantic outputs of them.
            The whole architecture is shown in the following figure:
          </p>
          <div class="has-text-centered" style="margin: 30px 0;">
            <img src="static/images/pam-pipeline.jpg" alt="pam-pipeline" style="max-width: 100%; height: auto; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>

          <h4 class="title is-4" style="margin-top: 20px;">Semantic Perceiver</h4>

          <p>
            The architecture of Semantic Perceiver mirrors the SAM 2 Feature Fusing module (S2-FFM), employing a lightweight two-layer transformer with self-attention, cross-attention, and a point-wise MLP. 
            Specifically, it receives two primary inputs:
            enhanced mask tokens from S2-FFM, which incorporate IoU and prompt tokens information and serve as unique identifiers for precise mask generation;
            and updated image embeddings after S2-FFM, capturing general visual context and implicit features enriched through interaction with mask tokens. 
          </p>
          <p>
            Then, we concatenate N<sub>s</sub> learnable semantic tokens with the enhanced mask tokens.Finally, through further attention mechanisms within the Semantic Perceiver, we can fetch visual tokens rich in both general visual and object-level localization information.
            The Semantic Perceiver is illustrated in the following figure:
          </p>
          <div class="has-text-centered" style="margin: 30px 0;">
            <img src="static/images/semantic-perceiver-pipeline.png" alt="Semantic Perceiver" style="max-width: 100%; height: auto; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>

          <h4 class="title is-4" style="margin-top: 20px;">Streaming Video Encode and Decode</h4>
          <p>
            Building upon the progressive introduction of historical information per frame via memory modules in SAM 2, we propose a straightforward strategy for region-level streaming video captioning without adding complex components.
            Specifically, an additional 2 &times 2 pixel shuffle operation is applied to the last frame of each video clip.
            This leads to a greater density of visual tokens, improving the preservation of historical visual information. 
            These tokens subsequently act as the initial frame for the next video clip and are processed by the LLM together with the remaining frames of that clip.
            This approach ensures that each clip is processed consistently and effectively passes crucial historical information from the previous clip into the next video clip.
            Additionally, we incorporate the previous textual description into the prompt to further augment contextual history, enhancing the model's comprehension and descriptive accuracy for ongoing events.
            In practice, our framework allows users to flexibly specify decode timestamps. Upon reaching a designated timestamp, the model describes the specified region within the temporal interval between the current timestamp and the previous one.
          </p>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- DATA -->
<section class="section" style="margin-top: 40px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3"><img src="static/images/pam-data-logo.png" alt="Icon" style="height: 1.6em; vertical-align: middle; margin-right: 0.0em;"> <span class='tiifbench'>PAM Data</span></h2>
        <div class="content has-text-justified">
          <p>
            To enhance PAM's comprehensive visual perception capabilities, we develop a robust data refinement and augmentation pipeline to curate a high-quality training dataset. This dataset is distinguished by three key features: 
          </p>
          <p>
            <b>(1) Broad-ranging Semantic Granularities.</b> It provides diverse visual semantic annotations spanning from coarse-level (categories, definitions, contextual functionalities) to fine-grained (detailed descriptions).
          </p>
          <p>
            <b>(2) Regional Streaming Caption Annotations.</b> The first dataset to curate annotations specifically for streaming video region captioning.
          </p>
          <p>
            <b>(3) Bilingual Annotations,</b> supporting both English and Chinese.
          </p>
          <p>
            The construction pipeline is shown in the following figure:
          </p>
          <div class="has-text-centered" style="margin: 30px 0;">
            <img src="static/images/build_data.png" alt="data-pipeline" style="max-width: 100%; height: auto; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>
          <p>
            All public dataset collection are shown in the following figure:
          </p>
          <div class="has-text-centered" style="margin: 30px 0;">
            <img src="static/images/data-collection.png" alt="data-pipeline" style="max-width: 100%; height: auto; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>

          <h4 class="title is-4" style="margin-top: 20px;">Image Dataset</h4>
          <p>
            <b>Regional Recognition, Explanation, and Caption.</b> For regional recognition, we utilize multiple instance detection and segmentation datasets 
            along with scene text recognition datasets. 
            In this context, the bounding box or mask serves as the visual prompt input, and the label is treated as the output.
          </p>
          <p>
            To achieve deep, fine-grained visual understanding beyond simple classification, we propose an enhanced pipeline that generates:
            clear conceptual explanations, contextual functional roles, and detailed descriptions for each specific region.
            This multi-dimensional information aims to significantly improve user comprehension, particularly for uncommon terms or unfamiliar subjects.
            To implement this, we utilize the latest VLMs for their extensive world knowledge and powerful visual understanding capabilities to assist refinement.
            Specifically, we apply the Set of Mask (SoM) method to identify regions of interest, and use original annotations as context to guide models to produce desired responses, which then undergo manual quality assurance.
          </p>

          <h4 class="title is-4" style="margin-top: 20px;">Video Dataset</h4>
          <p>
            <b>Region-level Video Caption.</b> To extend the model's regional captioning capabilities to video, we collected and analyzed several existing video datasets, including referring detection and segmentation datasets
            as well as the recent Sa2VA annotations for the SA-V dataset. These datasets, designed for detecting, segmenting, and captioning specific objects in videos based on textual descriptions, often contain descriptions that are overly coarse, simplistic, inaccurate, or predominantly static, neglecting essential temporal details such as object motion, interactions, and state changes throughout the video.
            To address the existing limitations, we propose the <b>storyboard-driven caption expansion</b> method.
            This process involves several key stages:
            <p>
              <b>(1) Keyframe Sampling:</b> Six keyframes are uniformly extracted from each video.
            </p>
            <p>
              <b>(2) Storyboard Synthesis:</b> These extracted keyframes are combined to form a high-resolution composite image, presented in a storyboard format.
            </p>
            <p>
              <b>(3) Object-Centric Highlighting:</b> Within this composite image, each individual frame specifically highlights the target object using a colored bounding box or mask, implemented by SoM.
            </p>
            <p>
              <b>(4) LLM-Powered Elaboration:</b> Then, using the original annotations as condition, we prompt GPT-4o to generate descriptions that are both refined, detailed and temporally aware. 
            </p>
          </p>
          <p>This multi-frame consolidation is critical as it enhances GPT-4o's contextual comprehension, yielding superior descriptions compared to individual frame analysis.</p>

          <p>
            <b>Region-level Streaming Video Caption.</b> Beyond describing the entire video, we aim to extend the model's capabilities to a streaming manner.
            To achieve this, we perform additional augmentation on our refined region-level video caption data.
            Specifically, we first employ the TRACE-Uni model to segment the input video into multiple distinct events, each demarcated by its temporal boundaries.
            Subsequently, for each segmented video clip, we apply the same `storyboard-driven' processing method. To generate precise and continuous event descriptions, the GPT-4o input prompt was redesigned to iteratively incorporate the description from the preceding video clip as contextual information for processing the current clip.

          </p>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- Experiments -->
<section class="section" style="margin-top: 40px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <p>
            <img src="static/images/pam-logo.png" alt="Icon" style="height: 1.6em; vertical-align: middle; margin-right: 0.0em;"> <b>PAM</b>
             provides various semantic granularities informantion and support bilingual outputs:
          </p>
          <div class="has-text-centered" style="margin: 30px 0;">
            <img src="static/images/img-example.png" alt="experiments" style="max-width: 100%; height: auto; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>
          <p>
            <img src="static/images/pam-logo.png" alt="Icon" style="height: 1.6em; vertical-align: middle; margin-right: 0.0em;"> <b>PAM</b>
              provides region-level non-streaming and streaming video bilingual caption:
          </p>
          <div class="has-text-centered" style="margin: 30px 0;">
            <img src="static/images/video-example.png" alt="experiments" style="max-width: 100%; height: auto; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>
          <p>
            <img src="static/images/pam-logo.png" alt="Icon" style="height: 1.6em; vertical-align: middle; margin-right: 0.0em;"> <b>PAM</b>
             does well on both img recognition/captioning and video captioning benchmark:
          </p>
          <div class="has-text-centered" style="margin: 30px 0; margin-bottom: -20px;">
            <img src="static/images/table1.png" alt="experiments" style="max-width: 100%; height: auto; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>
          <div class="has-text-centered" style="margin: 30px 0; margin-bottom: -20px;">
            <img src="static/images/table2.png" alt="experiments" style="max-width: 100%; height: auto; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>
          <div class="has-text-centered" style="margin: 30px 0;">
            <img src="static/images/table3.png" alt="experiments" style="max-width: 100%; height: auto; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>

          <p>
            <img src="static/images/pam-logo.png" alt="Icon" style="height: 1.6em; vertical-align: middle; margin-right: 0.0em;"> <b>PAM</b>
             also demonstrates superior inference efficiency and requires less GPU memory for both image and video processing, highlighting its suitability for efficient deployment in real-world applications:
          </p>
          <div class="has-text-centered" style="margin: 30px 0;">
            <img src="static/images/efficiency.png" alt="experiments" style="max-width: 100%; height: auto; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX" style="margin-top: 175px;">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @misc{
      }
    </code></pre>
  </div>
</section>


<!-- Affiliation -->
<section class="section" style="margin-top: 0px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <div class="content">
          <!-- Á¨¨‰∏ÄÊéíÂõæÁâá -->
          <div class="columns is-centered is-mobile" style="margin-bottom: 30px; align-items: center;">
            <div class="column is-narrow" style="padding: 10px 20px;">
              <img src="static/images/cuhk.png" alt="cuhk" style="height: 110px;">
            </div>
            <div class="column is-narrow" style="padding: 10px 30px;">
              <img src="static/images/pku.png" alt="pku" style="height: 75px;">
            </div>
            <div class="column is-narrow" style="padding: 10px 30px;">
              <img src="static/images/hku.jpg" alt="hku" style="height: 110px;">
            </div>
            <div class="column is-narrow" style="padding: 10px 30px;">
              <img src="static/images/polyu.png" alt="polyu" style="height: 110px;">
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/js/bulma-carousel.min.js"></script>
<script>
	bulmaCarousel.attach('#results-carousel', {
		slidesToScroll: 1,
		slidesToShow: 1,
        autoplay: true, 
        autoplaySpeed: 3000,
        loop: true
	});
</script>

</body>
</html>
